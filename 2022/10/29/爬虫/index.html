<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="python">
    <meta name="description" content="欢迎来到我的笔记空间">
    <meta name="author" content="Xu">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-redefine.png">
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2022/10/29/爬虫/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    <meta property="og:type" content="article">
    <meta property="og:title" content="爬虫">
    <meta property="og:description" content="欢迎来到我的笔记空间">
    <meta property="og:url" content="http://example.com2022/10/29/爬虫/">
    <meta property="og:image" content="/images/redefine-logo.svg">
    <meta property="og:site_name" content="Xu">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="爬虫">
    <meta name="twitter:description" content="欢迎来到我的笔记空间">
    <meta name="twitter:image" content="/images/redefine-logo.svg">
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-logo.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-logo.svg">
    <meta name="theme-color" content="#005080">
    <link rel="shortcut icon" href="/images/redefine-logo.svg">
    
    <title>
        
            爬虫 -
        
        Xu
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    
    
    
    
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/redefine-avatar.svg","favicon":"/images/redefine-logo.svg","article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"https://evan.beee.top/img/wallhaven-wqery6-light.webp","dark":"https://evan.beee.top/img/wallhaven-wqery6-dark.webp"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"Xu","custom_font":{"enable":false,"font_family":null,"font_url":null}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":false,"preload":true},"code_block":{"copy":true,"style":"mac","custom_font":{"enable":false,"font_family":null,"font_url":null}},"pjax":{"enable":true},"lazyload":{"enable":true},"friend_links":{"columns":2},"home_article":{"date_format":"auto","category":{"enable":true,"limit":3},"tag":{"enable":true,"limit":3}},"plugins":{"aplayer":{"enable":false,"audio":[{"name":null,"artist":null,"url":null,"cover":null},{"name":null,"artist":null,"url":null,"cover":null}]}}};
    REDEFINE.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
    
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="menu-wrapper">
    
    <div class="menu-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Xu
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="pc">
                <ul class="menu-list">
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="" 
                                    target="_blank" rel="noopener" href="https://status.evanluo.top/"  >
                                    
                                        
                                            <i class="fa-regular fa-chart-bar"></i>
                                        
                                        状态
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">我
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://code-xushuai.github.io">BLOG
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/friends">FRIENDS
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="menu-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        友链&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://roweller.github.io/">ROWLLER
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://Krisxf.github.io/">KRISXF
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://redefine-docs.ohevan.com/docs/intro">主题文档
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="menu-drawer">
        <ul class="drawer-menu-list">
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="" 
                        target="_blank" rel="noopener" href="https://status.evanluo.top/"  >
                             
                                
                                    <i class="fa-regular fa-chart-bar"></i>
                                
                                状态
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/about">我</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://code-xushuai.github.io">BLOG</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/friends">FRIENDS</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-menu-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                友链&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://roweller.github.io/">ROWLLER</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://Krisxf.github.io/">KRISXF</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://redefine-docs.ohevan.com/docs/intro">主题文档</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">爬虫</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/redefine-avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Xu</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="pc">2022-10-29 17:08:03</span>
        <span class="mobile">2022-10-29 17:08</span>
        <span class="hover-info">创建时间</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="pc">2023-03-06 16:48:43</span>
            <span class="mobile">2023-03-06 16:48</span>
            <span class="hover-info">更新时间</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Python/">Python</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="12306模拟登录编码流程"><a class="markdownIt-Anchor" href="#12306模拟登录编码流程">#</a> 12306 模拟登录编码流程:</h2>
<ul>
<li>使用 selenium 打开登录页面</li>
<li>对当前 selenium 打开的这张页面进行截图</li>
<li>对当前图片局部区域（验证码图片）进行裁剪
<ul>
<li>好处：将验证码图片和模拟登录进行一一对应。、</li>
<li>使用超级鹰识别验证码图片 (坐标)<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/107.png"
                      alt=""
                ></li>
</ul>
</li>
<li>余票检测</li>
</ul>
<h2 id="梨视频爬取思路"><a class="markdownIt-Anchor" href="#梨视频爬取思路">#</a> 梨视频爬取思路</h2>
<ul>
<li>将每一个视频详情页的 url 进行解析</li>
<li>对视频详情页的 url 进行请求发送</li>
<li>在视频详情页的页面源码中进行全局搜索，发现没有找到 video 标签
<ul>
<li>视频标签是动态加载出来</li>
<li>动态加载的数据方式
<ul>
<li>ajax</li>
<li>js</li>
</ul>
</li>
</ul>
</li>
<li>在页面源码中搜索.mp4，定位到了视频的地址 (存在于一组 js 代码)
<ul>
<li>通过正则将视频地址解析出来对其发起请求即可</li>
</ul>
</li>
</ul>
<h2 id="异步爬虫"><a class="markdownIt-Anchor" href="#异步爬虫">#</a> 异步爬虫</h2>
<ul>
<li>基于线程池
<ul>
<li>from multiprocessing.dummy import Pool</li>
<li>map(callback,alist)
<ul>
<li>可以使用 callback 对 alist 中的每一个元素进行指定形式的异步操作</li>
</ul>
</li>
<li>同步<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/32.jpg"
                      alt=""
                ></li>
<li>异步<br>
需要导包：from multiprocessing.dummy import Pool<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/33.jpg"
                      alt=""
                ></li>
</ul>
</li>
<li>基于单线程 + 多任务的异步爬虫  pip install asyncio
<ul>
<li>特殊的函数
<ul>
<li>如果一个函数的定义被 async 修饰后，则该函数就变成了一个特殊的函数</li>
<li>特殊之处:
<ul>
<li>该特殊的函数调用后，函数内部的实现语句不会被立即执行</li>
<li>该特殊函数被调用后会返回一个协程对象</li>
</ul>
</li>
</ul>
</li>
<li>协程对象、
<ul>
<li>对象。通过特殊函数的调用返回一个协程对象。</li>
<li>协程 == 特殊函数 == 一组指定的操作</li>
<li>协程 == 组指定的操作</li>
</ul>
</li>
<li>任务对象
<ul>
<li>任务对象就是一个高级的协程对象。(任务对象就是对协程对象的进一步封装）</li>
<li>任务 == 协程 == 特殊函数 == 组指定操作</li>
<li>任务 == 组指定的操作</li>
<li>- 如何创建一个任务对象:
<ul>
<li>asyncio.ensure_future (协程对象)</li>
</ul>
</li>
</ul>
</li>
<li>事件循环对象</li>
<li>对象。</li>
<li>作用:
<ul>
<li>可以将多个任务对象注册 / 装载到事件循环对象中</li>
<li>如果开启了事件循环后，则其内部注册 / 装载的任务对象表示的指定操作就会基于异步的被执行<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/102.png"
                      alt=""
                ></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="分析"><a class="markdownIt-Anchor" href="#分析">#</a> 分析</h2>
<ul>
<li>处理乱码后，页面显示【异常访问请求】导致请求数据的缺失。
<ul>
<li>异常的访问请求<br>
网站后台已经检测出该次请求不是通过浏览器发起的请求而是通过爬虫程序发起的请求。(不是通过浏览器发起的请求都是异常请求 - 网站的后台是如何知道请求是不是通过浏览器发起的呢？</li>
<li>网站的后台是如何知道请求是不是通过浏览器发起的呢？<br>
 是通过判定请求的请求头中的 user-agent 判定的</li>
<li>什么是 User-Agent
<ul>
<li>请求载体的身份标识</li>
<li>什么是请求载体:
<ul>
<li>浏览器<br>
浏览器的身份标识是统一固定，身份标识可以从抓包工具中获取</li>
<li>爬虫程序。<br>
身份标识是各自不同</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>分析浏览器开发者工具中 Elements 和 network 这两个选项卡对应的页面源码数据有何不同之处？
<ul>
<li>Elements 中包含的显示的页面源码数据为当前页面所有的数据加载完毕后对应的完整的页面源码数据 (包含了动态加载数据)</li>
<li>network 中显示的页面源码数据仅仅为某一个单独的请求对应的响应数据 (不包含动态加载数据)</li>
<li>结论：如果在进行数据解析的时候，一定是需要对页面布局进行分析，如果当前网站没有动态加载的数据就可以直接使用 Elements 对页面布局进行分析，否则只可以使用 network 对页面数据进行分析。</li>
</ul>
</li>
<li>动态加载数据的捕获
<ul>
<li>什么叫做动态加载的数据？<br>
1. 我们通过 requests 模块进行数据爬取无法每次都实现可见即可得。<br>
2. 有些数据是通过非浏览器地址栏中的 url 请求到的数据，而是其他请求请求到的数据，那么这些通过其他请求请求到的数据就是动态加载的数据。</li>
<li>如何检测网页中是否存在动态加载数据？
<ul>
<li>基于抓包工具进行局部搜索。<br>
在当前网页中打开抓包工具，捕获到地址栏的 url 对应的数据包，在该数据包的 response 选项卡搜索我们想要爬取的数据，如果搜索到了结果则表示数据不是动态加载的</li>
</ul>
</li>
</ul>
</li>
<li>模拟登录时：在请求参数中如果看到了一组乱序的请求参数，最好去验证码这组请求参数是否为动态变化。
<ul>
<li>方式 1∶常规来讲一半动态变化的请求参数会被隐藏在前台页面中，那么我们就要去前台页面源码中取找。</li>
<li>方式 2: 如果前台页面没有的话，我们就可以基于抓包工具进行全局搜索。</li>
</ul>
</li>
</ul>
<h2 id="request"><a class="markdownIt-Anchor" href="#request">#</a> request</h2>
<ul>
<li>参数动态化<br>
如果请求的 url 携带参数，且我们想要将携带的参数进行动态化操作:<br>
1. 将携带的动态参数以键值对的形式封装到一个字典中<br>
 2. 将该字典作用到 get 方法的 params 参数中即可<br>
 3. 需要将原始携带参数的 ur1 中将携带的参数删除</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#  https://www.baidu.com/s?wd=wo</span></span><br><span class="line">word = <span class="built_in">input</span>(<span class="string">&quot;请输入：&quot;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s&#x27;</span></span><br><span class="line">params = &#123;</span><br><span class="line">    kw: word</span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url=url, params=params)</span><br><span class="line"><span class="comment"># response = requests.post(url=url, data=data, headers=headers)   必须传参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决编码问题</span></span><br><span class="line"><span class="comment"># 修改响应数据的编码格式</span></span><br><span class="line">reponse.encoding = <span class="string">&#x27;utf-8&#x27;</span> </span><br><span class="line"></span><br><span class="line">content = response.text  <span class="comment"># 字符串形式响应对象</span></span><br></pre></td></tr></table></figure></div>
<br>
<h2 id="urllib"><a class="markdownIt-Anchor" href="#urllib">#</a> urllib</h2>
<ul>
<li>基本使用  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个url：访问的网址</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url=<span class="string">&quot;http://www.baidu.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#模拟浏览器向服务器发送请求,用response接收</span></span><br><span class="line">response=urllib.request.urlopen(url)</span><br><span class="line">       </span><br><span class="line"><span class="comment">#获取相应的内容，read() </span></span><br><span class="line">content=respone.read().decode(<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure></div>
</li>
<li>状态码（判断代码是否正确）：respone.getcode ()<br>
 返回 200 为正确</li>
<li>下载  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span></span><br><span class="line"><span class="comment">#网页下载</span></span><br><span class="line">url_page=<span class="string">&quot;&quot;</span></span><br><span class="line">urllib.request.urlretrieve(url_page,<span class="string">&quot;百度一下.html&quot;</span>)</span><br><span class="line"><span class="comment">#图片下载</span></span><br><span class="line">url_img=<span class="string">&quot;&quot;</span></span><br><span class="line">urllib.request.urlretrieve(url_img,<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line"><span class="comment">#视频下载</span></span><br><span class="line">url_video=<span class="string">&quot;&quot;</span></span><br><span class="line">urllib.request.urlretrieve(url_video,<span class="string">&quot;1.mp4&quot;</span>)</span><br></pre></td></tr></table></figure></div>
</li>
</ul>
<hr>
<br>
<h2 id="ua反爬"><a class="markdownIt-Anchor" href="#ua反爬">#</a> UA 反爬</h2>
<pre><code><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url=<span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">headers=&#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为urlopen不能接收字典，因此headers不能直接传进去</span></span><br><span class="line"><span class="comment">#请求对象的定制：解决反爬的第一种手段</span></span><br><span class="line">request=urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取相应</span></span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line">       </span><br><span class="line"><span class="comment">#获取相应的内容</span></span><br><span class="line">content=response.read().decode(<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure></div>
</code></pre>
<hr>
<br>
<h2 id="图片下载两种方式"><a class="markdownIt-Anchor" href="#图片下载两种方式">#</a> 图片下载两种方式</h2>
<ul>
<li>方式一</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一</span></span><br><span class="line">url = <span class="string">&#x27;https://img1.baidu.com/it/u=413643897,2296924942&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=800&amp;h=500&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Mobile Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url=url, headers=headers)</span><br><span class="line">img_data = response.content  <span class="comment"># content返回的是二进制形式的响应数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(img_data)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>方式二</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#图片下载</span></span><br><span class="line">url_img=<span class="string">&quot;&quot;</span></span><br><span class="line">urllib.request.urlretrieve(url_img,<span class="string">&quot;1.jpg&quot;</span>)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>区别<br>
 requests 可以进行 UA 伪装</li>
</ul>
<h2 id="urllibparesquote方法将汉字转换成unicode编码"><a class="markdownIt-Anchor" href="#urllibparesquote方法将汉字转换成unicode编码">#</a> urllib.pares.quote () 方法：将汉字转换成 unicode 编码</h2>
<pre><code><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url=<span class="string">&quot;https://www.baidu.com/s?wd=&quot;</span></span><br><span class="line"><span class="comment">#将汉字转换成unicode编码</span></span><br><span class="line">name=urllib.parse.quote(<span class="string">&quot;周杰伦&quot;</span>)</span><br><span class="line"><span class="comment">#更新url</span></span><br><span class="line">url+=name</span><br><span class="line"><span class="comment">#找到网页的UA</span></span><br><span class="line">headers=&#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#定制请求对象</span></span><br><span class="line">request=urllib.request.Request(url=url,headers=headers)</span><br><span class="line"><span class="comment">#模拟浏览器向服务器发送请求</span></span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line"><span class="comment">#接收请求</span></span><br><span class="line">content=response.read().decode(<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure></div>
</code></pre>
<hr>
<br>
<h2 id="urllibparseurlencode将多个参数同时转换成unicode编码并且用进行连接"><a class="markdownIt-Anchor" href="#urllibparseurlencode将多个参数同时转换成unicode编码并且用进行连接">#</a> urllib.parse.urlencode ()：将多个参数同时转换成 unicode 编码并且用 &amp; 进行连接</h2>
<pre><code><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">base_url=<span class="string">&quot;https://www.baidu.com/s?&quot;</span></span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>:<span class="string">&#x27;周杰伦&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span>:<span class="string">&#x27;男&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;location&#x27;</span>:<span class="string">&#x27;中国台湾省&#x27;</span> </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#传入参数为字典</span></span><br><span class="line">new_url=urllib.parse.urlencode(data)</span><br><span class="line"><span class="comment">#新的请求路径</span></span><br><span class="line">url=base_url+new_url</span><br></pre></td></tr></table></figure></div>
</code></pre>
<hr>
<br>
<h2 id="post请求"><a class="markdownIt-Anchor" href="#post请求">#</a> post 请求</h2>
<pre><code><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url=<span class="string">&quot;https://fanyi.baidu.com/sug&quot;</span></span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>:<span class="string">&#x27;spider&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#post请求必须进行编码</span></span><br><span class="line">data=urllib.parse.urlencode(data).encode(<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#post请求参数放在请求定制对象中</span></span><br><span class="line">request=urllib.request.Request(url=url,data=data,headers=headers)</span><br><span class="line"><span class="comment">#模拟浏览器向服务器发送请求</span></span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line">content=response.read().encode(<span class="string">&#x27;UTF-8&#x27;</span>)</span><br></pre></td></tr></table></figure></div>
</code></pre>
<hr>
<br>
<h2 id="ajax-get请求豆瓣电影数据获取"><a class="markdownIt-Anchor" href="#ajax-get请求豆瓣电影数据获取">#</a> ajax get 请求（豆瓣电影数据获取）</h2>
<ul>
<li>
<p>一页</p>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url=<span class="string">&quot;https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20&quot;</span></span><br><span class="line"><span class="comment">#任意headers</span></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#请求对象定制</span></span><br><span class="line">request=urllib.request.Request(url=url,headers=headers);</span><br><span class="line"><span class="comment">#获取相应</span></span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line"><span class="comment">#内容</span></span><br><span class="line">content=response.read().decode(<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载本地，open默认使用gbj编码</span></span><br><span class="line">f=<span class="built_in">open</span>(<span class="string">&quot;douban.json&quot;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">f.write(content)</span><br></pre></td></tr></table></figure></div>
</li>
<li>
<p>多页</p>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">creat_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&quot;https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;&quot;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data=&#123;</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>:(page-<span class="number">1</span>)*<span class="number">20</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>:<span class="number">20</span></span><br><span class="line">    &#125;</span><br><span class="line">    url=base_url+urllib.parse.urlencode(data)</span><br><span class="line">    request=urllib.request.Request(url=url,headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page,content</span>):</span><br><span class="line">    f=<span class="built_in">open</span>(<span class="string">&quot;douban_&quot;</span>+<span class="built_in">str</span>(page)+<span class="string">&quot;.json&quot;</span>,<span class="string">&quot;w&quot;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    f.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page =<span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入开始页数：&quot;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入结束页数：&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page,end_page+<span class="number">1</span>):</span><br><span class="line">        <span class="comment">#每一页都要定制请求对象creat_request(page)</span></span><br><span class="line">        request=creat_request(page)</span><br><span class="line">        <span class="comment">#获取相应</span></span><br><span class="line">        response=urllib.request.urlopen(request)</span><br><span class="line">        <span class="comment">#内容</span></span><br><span class="line">        content=response.read().decode(<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">        <span class="comment">#下载</span></span><br><span class="line">        down_load(page,content)</span><br></pre></td></tr></table></figure></div>
</li>
</ul>
<hr>
<br>
<h2 id="ajax-post请求肯德基找出每页的规律"><a class="markdownIt-Anchor" href="#ajax-post请求肯德基找出每页的规律">#</a> ajax post 请求 — 肯德基 (找出每页的规律)</h2>
<ul>
<li>分页数据的爬取操作<br>
爬取肯德基的餐厅位置数据</li>
<li>分析:
<ul>
<li>1. 在录入关键字的文本框中录入关键字按下搜索按钮，发起的是一个 ajax 请求<br>
当前页面刷新出来的位置信息一定是通过 ajax 请求请求到的数据</li>
<li>2. 基于抓包工具定位到该 ajax 请求的数据包，从该数据包中捕获到:
<ul>
<li>请求的 url</li>
<li>请求方式</li>
<li>请求携带的参数</li>
<li>看到响应数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 肯德基</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Mobile Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;keyword&#x27;</span>: <span class="string">&#x27;重庆&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pageIndex&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(url=url, data=data, headers=headers)</span><br><span class="line">content = response.json()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> content[<span class="string">&#x27;Table1&#x27;</span>]:</span><br><span class="line">    name = i[<span class="string">&#x27;storeName&#x27;</span>]</span><br><span class="line">    address = i[<span class="string">&#x27;addressDetail&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br><span class="line">    <span class="built_in">print</span>(address+<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>多页</li>
</ul>
<h2 id="-code12-"><a class="markdownIt-Anchor" href="#-code12-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">creat_request</span>(<span class="params">page</span>):</span><br><span class="line">    url=<span class="string">&quot;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&quot;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data=&#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;重庆&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27; &#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&quot;10&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    data=urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    request=urllib.request.Request(url=url,headers=headers,data=data)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">page,content</span>):</span><br><span class="line">    f=<span class="built_in">open</span>(<span class="string">&quot;地址_&quot;</span>+<span class="built_in">str</span>(page)+<span class="string">&#x27;.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    f.write(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page=<span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入开始页码: &#x27;</span>))</span><br><span class="line">    end_page=<span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码: &#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page,end_page+<span class="number">1</span>):</span><br><span class="line">        <span class="comment">#定制每一页请求对象</span></span><br><span class="line">        request = creat_request(page)</span><br><span class="line">        <span class="comment">#获取相应</span></span><br><span class="line">        response=urllib.request.urlopen(request)</span><br><span class="line">        <span class="comment">#接收内容</span></span><br><span class="line">        content=response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="comment">#下载</span></span><br><span class="line">        down_load(page,content)</span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="异常"><a class="markdownIt-Anchor" href="#异常">#</a> 异常</h2>
<pre><code><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">except</span> urllib.error.HttpError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;系统正在升级&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
</code></pre>
<hr>
<br>
<h2 id="cookie"><a class="markdownIt-Anchor" href="#cookie">#</a> cookie</h2>
<ul>
<li>使用场景：数据采集中，需要绕开登录，进入到某个页面</li>
<li>cookie: 携带登录信息，如果有登录后的 cookie，那就可以登录任意页面</li>
<li>referer: 判断当前路径是不是由上一个路径进来的，一般用作图片防盗链</li>
<li>因为请求头的信息不够，因此访问不成功</li>
<li>携带 cookie 发请求</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session = request.Session()</span><br><span class="line"><span class="comment"># 后面的request对象换成session</span></span><br></pre></td></tr></table></figure></div>
<h2 id="-code15-"><a class="markdownIt-Anchor" href="#-code15-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url=<span class="string">&quot;https://user.qzone.qq.com/2578393167/infocenter&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#x27;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cache-control&#x27;</span>: <span class="string">&#x27;max-age=0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>: <span class="string">&#x27;2578393167_todaycount=0; 2578393167_totalcount=2183; RK=YVvUR0JQ12; ptcz=c627979381dfc7a30c18832d7fb11cc2a5570256a6943af7e90b25439b6c0feb; pgv_pvid=8367386128; tvfe_boss_uuid=bf664e715257f060; o_cookie=2578393167; qz_screen=1536x864; QZ_FE_WEBP_SUPPORT=1; uin=o2578393167; skey=@gkZqZO2fy; p_uin=o2578393167; Loading=Yes; pgv_info=ssid=s8446429120; pt4_token=FR7N-PhJ3XrcHbADF9QUKxTeM18cRk0qudgiPqWD1lU_; p_skey=3A-LodRdokqoPcKgd0VuwAOOtAPDi7kntBosX8jeieI_; cpu_performance_v8=3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;if-modified-since&#x27;</span>: <span class="string">&#x27;Wed, 02 Nov 2022 15:11:28 GMT&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>:<span class="string">&#x27; &quot;Microsoft Edge&quot;;v=&quot;107&quot;, &quot;Chromium&quot;;v=&quot;107&quot;, &quot;Not=A?Brand&quot;;v=&quot;24&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>: <span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>: <span class="string">&quot;Windows&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>: <span class="string">&#x27;document&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>: <span class="string">&#x27;navigate&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>: <span class="string">&#x27;none&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-user&#x27;</span>: <span class="string">&#x27;?1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;upgrade-insecure-requests&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.26&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;wangye.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f.write(content)</span><br></pre></td></tr></table></figure></div></h2>
<ul>
<li>cookie 验证码
<ul>
<li>
<p>验证码识别</p>
<ul>
<li>基于线上的打码平台识别验证码</li>
<li>打码平台
<ul>
<li>超级鹰 (使用): <a class="link"   target="_blank" rel="noopener" href="http://www.chaojiying.com/about.html" >http://www.chaojiying.com/about.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><br>
1. 注册【用户中心的身份】<br>
2. 登录 (用户中心的身份)<br>
 1. 查询余额，请充值<br>
 2. 创建一个软件 ID (899370)<br>
 3. 下载示例代码</li>
</ul>
</li>
</ul>
</li>
<li>
<p>古诗文网案例<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/101.png"
                      alt=""
                ></p>
</li>
</ul>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># (1)登录页面地址</span></span><br><span class="line">url = <span class="string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># (2)获取__VIEWSTATE和__VIEWSTATEGENERATOR的值就要解析网页源码</span></span><br><span class="line">response = requests.get(url=url,headers=headers)</span><br><span class="line">content = response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用bs4解析网页源码获取__VIEWSTATE和__VIEWSTATEGENERATOR</span></span><br><span class="line">soup = BeautifulSoup(content,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># 获取__VIEWSTATE</span></span><br><span class="line">viewstate = soup.select(<span class="string">&#x27;#__VIEWSTATE&#x27;</span>)[<span class="number">0</span>].attrs.get(<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line"><span class="comment"># 获取__VIEWSTATEGENERATO</span></span><br><span class="line">viewstategenrator = soup.select(<span class="string">&#x27;#__VIEWSTATEGENERATOR&#x27;</span>)[<span class="number">0</span>].attrs.get(<span class="string">&#x27;value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (3)获取图片验证码  注意：不能用urllib中的下载图片操作，因为该验证码不是此次登录时的</span></span><br><span class="line">code = soup.select(<span class="string">&#x27;#imgCode&#x27;</span>)[<span class="number">0</span>].attrs.get(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">code_url = <span class="string">&#x27;https://so.gushiwen.cn&#x27;</span> + code</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过session返回值把请求变成一个对象</span></span><br><span class="line">session = requests.session()</span><br><span class="line">response_code = session.get(code_url)</span><br><span class="line"><span class="comment"># 不能时text，应为图片是二进制的</span></span><br><span class="line">content_code = response_code.content</span><br><span class="line"><span class="comment"># wb:将二进制写入到文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;code.jpg&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content_code)</span><br><span class="line"></span><br><span class="line">code_name = <span class="built_in">input</span>(<span class="string">&#x27;请输入：&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (4)登录</span></span><br><span class="line">url_post = <span class="string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx&#x27;</span></span><br><span class="line">data_post= &#123;</span><br><span class="line">    <span class="string">&#x27;__VIEWSTATE&#x27;</span>:viewstate,</span><br><span class="line">    <span class="string">&#x27;__VIEWSTATEGENERATOR&#x27;</span>:viewstategenrator,</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;http://so.gushiwen.cn/user/collect.aspx&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>:  <span class="string">&#x27;2578393167@qq.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pwd&#x27;</span>:  <span class="string">&#x27;123456&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;code&#x27;</span>:  code_name,</span><br><span class="line">    <span class="string">&#x27;denglu&#x27;</span>: <span class="string">&#x27;登录&#x27;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">response_post = session.get(url=url_post,headers=headers,data=data_post)</span><br><span class="line">content_post = response_post.text</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;g.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f.write(content_post)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
</li>
</ul>
<br>
<h2 id="handler处理"><a class="markdownIt-Anchor" href="#handler处理">#</a> Handler 处理</h2>
<ul>
<li>为什么要 handler
<ul>
<li>urllib.request.urlopen (url): 不能定制请求头</li>
<li>urllib.request.Request (url=url,headers=headers)：能定制请求头</li>
<li>handler ：能定制高级请求头</li>
</ul>
</li>
<li>解决动态 cookie 和代理问题</li>
<li>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url = url,headers = headers)</span><br><span class="line"><span class="comment">#  获取相应</span></span><br><span class="line"><span class="comment">#  handler   build_opener    open</span></span><br><span class="line"><span class="comment">#(1)  获得handler对象</span></span><br><span class="line">handler = urllib.request.HTTPHandler()</span><br><span class="line"><span class="comment">#(2)  获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"><span class="comment">#(3)  调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure></div>
</li>
<li>代理
<ul>
<li>常用功能
<ul>
<li>突破自身 IP 访问限制，访问国外站点</li>
<li>访问一些内网才能访问的网站（单位或集体资源）</li>
<li>提高访问速度</li>
<li>隐藏真实 IP</li>
</ul>
</li>
<li>代码配置代理
<ul>
<li>创建 Request 对象</li>
<li>创建 proxyHandler 对象</li>
<li>用 handler 对象创建 opener 对象</li>
<li>使用 opener.open () 方法 发送请求</li>
</ul>
</li>
<li></li>
</ul>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  urllib.request</span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?wd=ip&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url , headers=headers)</span><br><span class="line"><span class="comment">#代理(类型：ip:端口号)</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;HTTP&#x27;</span>:<span class="string">&#x27;223.96.90.216:8085&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#代理  handler  bulid_opener  open</span></span><br><span class="line">handler = urllib.request.ProxyHandler()</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&quot;IP.html&quot;</span> , <span class="string">&#x27;w&#x27;</span> , encoding=<span class="string">&#x27;utf-7&#x27;</span>)</span><br><span class="line">f.write(content)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>代理池  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">proxies_pool = [</span><br><span class="line">    &#123;<span class="string">&#x27;HTTP&#x27;</span>:<span class="string">&#x27;223.96.90.216:8085&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;HTTP&#x27;</span>:<span class="string">&#x27;223.96.90.216:8085&#x27;</span>&#125;</span><br><span class="line">]</span><br><span class="line">proxies = random.choice(proxies_pool)</span><br></pre></td></tr></table></figure></div>
</li>
</ul>
</li>
</ul>
<hr>
<br>
<h2 id="xpath基本使用"><a class="markdownIt-Anchor" href="#xpath基本使用">#</a> xpath 基本使用</h2>
<ul>
<li>解析
<ul>
<li>本地文件</li>
</ul>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html_tree = etree.parse(<span class="string">&#x27;xx.html&#x27;</span>)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>服务器相应文件 response.read ().decode (‘utf-8’)</li>
</ul>
  <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">html_tree = etree.HTML(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure></div>
</li>
<li>xpath 基本语法</li>
<li>//；查找所有的子孙节点</li>
<li>/：查找直接的子节点</li>
<li>具体内容：text ()</li>
<li></li>
</ul>
 <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">tree = etree.parse(<span class="string">&#x27;xx.html&#x27;</span>)</span><br><span class="line"><span class="comment">#查找&lt;ul&gt;下面的&lt;li&gt;</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&quot;//ul/li&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找所有id属性的li标签</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[@id]/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找id=a的li标签，注意id值加引号</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&quot;//yl/li[@id=&#x27;a&#x27;]/text()&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找id=a的li标签class的属性值</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[@id=a]/@class&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找id中包含有a的li标签</span></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[contains(@id,&#x27;</span>a<span class="string">&#x27;)]/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找id以a开头的li标签</span></span><br><span class="line"></span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul/li[starts-with(@id,&#x27;</span>a<span class="string">&#x27;)]/test()&#x27;</span>)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>站长素材</li>
</ul>
<h2 id="-code23-"><a class="markdownIt-Anchor" href="#-code23-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="comment"># https://sc.chinaz.com/tupian/fengjing.html</span></span><br><span class="line"><span class="comment"># https://sc.chinaz.com/tupian/fengjing_2.html</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">creat_request</span>(<span class="params">page</span>):</span><br><span class="line">    <span class="keyword">if</span>(page  == <span class="number">1</span>):</span><br><span class="line">        url = <span class="string">&#x27;https://sc.chinaz.com/tupian/fengjing.html&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://sc.chinaz.com/tupian/fengjing_&#x27;</span> +<span class="built_in">str</span>(page)+ <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_load</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="comment">#下载图片</span></span><br><span class="line">    tree = etree.HTML(content)</span><br><span class="line">    url_list = tree.xpath(<span class="string">&#x27;/html/body/div[3]/div[2]//img/@data-original&#x27;</span>)</span><br><span class="line">    name_list = tree.xpath(<span class="string">&#x27;/html/body/div[3]/div[2]//img/@alt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(name_list)):</span><br><span class="line">        url = url_list[i]</span><br><span class="line">        name = name_list[i]</span><br><span class="line">        url = <span class="string">&#x27;http:&#x27;</span> + url_list[i]</span><br><span class="line">        urllib.request.urlretrieve(url=url, filename=<span class="string">&#x27;./s/&#x27;</span>+name+<span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入开始页码&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;输入结束页码：&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page,end_page+<span class="number">1</span>):</span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">        request = creat_request(page)</span><br><span class="line"><span class="comment">#  获取网页源码</span></span><br><span class="line">        content = get_content(request)</span><br><span class="line"><span class="comment"># 下载</span></span><br><span class="line">        down_load(content)</span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="jsonpath"><a class="markdownIt-Anchor" href="#jsonpath">#</a> JsonPath</h2>
<ul>
<li>只能解析本地文件</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line"><span class="comment">#json.load()传入的是一个文件而不是文件名</span></span><br><span class="line">obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;xx.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#书的所有的作者</span></span><br><span class="line">auther_list = jsonpath.jsonpath(obj,<span class="string">&#x27;$.store.book[*].auther&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#所有作者</span></span><br><span class="line">auther_list = jsonpath.jsonpath(obj,<span class="string">&quot;$..auther&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<ul>
<li><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/9.png"
                      alt=""
                ></li>
<li>淘票票案例</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line">url = <span class="string">&#x27;https://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1667700200435_108&amp;jsoncallback=jsonp109&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;bx-v&#x27;</span>: <span class="string">&#x27;2.2.3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>: <span class="string">&#x27;tracknick=kwnixj; enc=WJ9%2BY%2B9Ba0Og9OPZlsj3Zw1i1c3qht%2Bw9Vyr7od34qSHztLvbFiqB6ZMsvyhT8GO8LKnzyAjOCe6gjwEYPb7%2Bg%3D%3D; miid=869373093817191131; cna=RbgVGzUtRz0CAd9o+pRYSFQT; sgcookie=E100JzeLFL%2Bf172iWSGaHljqtkSypkfXy9kCkfRkrXu3vxRFsCw%2FuuyW6LqjfX%2BMpGaijsTQ48yW7dwgX%2FGO1kcKNhXgHmOeVcH8AZEMaFrQbiU%3D; uc3=vt3=F8dCv4tAaNn5I%2FjD9oI%3D&amp;nk2=CMDDE1Oi&amp;id2=UNN65%2FAq6NVfZw%3D%3D&amp;lg2=VFC%2FuZ9ayeYq2g%3D%3D; lgc=kwnixj; uc4=nk4=0%40CqXSgKYDCK2bs%2Bx%2BlThjSeo%3D&amp;id4=0%40UgQycF6cWdyK8U%2FNHubRis5oSdbC; _cc_=UtASsssmfA%3D%3D; thw=cn; t=100127af212c3fb91def9c3c224aede4; mt=ci=-1_0; xlly_s=1; _m_h5_tk=397699bd6077336626585171e156430b_1667651434112; _m_h5_tk_enc=d234c64c2c478eadd9ae6470e653d89b; cookie2=17b46973b6c5b12cd471e675c4656220; v=0; _tb_token_=f36e9e49d6e65; tb_city=110100; tb_cityName=&quot;sbG+qQ==&quot;; tfstk=ciUCBPx1qpvQhVbk-v1NUu4fPT3PZTXssMMLO9_EnxIeJ7FCicLqhN5zqd0rwf1..; l=eB_MRapuTUTKskc8BOfZourza77TYIRAguPzaNbMiOCPOy1H5sbhW6ruuuTMCnGVhsfvR3z0a8DMBeYBqC2sjqj4axom4yMmn; isg=BMvLHfWqLPVcTHAO4yF1hStnWm-1YN_iV0Hnbj3I_IphXOu-xTCHMjbyNlyy_Dfa&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>: <span class="string">&#x27;https://dianying.taobao.com/?spm=a1z21.3046609.city.1.32c0112aEL1KQl&amp;city=110100&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua&#x27;</span>: <span class="string">&#x27; &quot;Microsoft Edge&quot;;v=&quot;107&quot;, &quot;Chromium&quot;;v=&quot;107&quot;, &quot;Not=A?Brand&quot;;v=&quot;24&quot;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-mobile&#x27;</span>: <span class="string">&#x27;?0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-ch-ua-platform&#x27;</span>: <span class="string">&#x27;Windows&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-dest&#x27;</span>: <span class="string">&#x27;empty&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-mode&#x27;</span>: <span class="string">&#x27;cors&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sec-fetch-site&#x27;</span>: <span class="string">&#x27;same-origin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;x-requested-with&#x27;</span>: <span class="string">&#x27;XMLHttpRequest&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(url=url , headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">content = content.split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;)&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;淘票票.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;淘票票.json&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">regionName_list = jsonpath.jsonpath(obj,<span class="string">&#x27;$..regionName&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(regionName_list)</span><br></pre></td></tr></table></figure></div>
<hr>
<br>
<h2 id="bs4"><a class="markdownIt-Anchor" href="#bs4">#</a> bs4</h2>
<ul>
<li>基本语法</li>
</ul>
<h2 id="-code26-"><a class="markdownIt-Anchor" href="#-code26-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认打开文件的编码为gbk，所以在打开文件时要指定编码格式</span></span><br><span class="line">soup = BeautifulSoup(<span class="built_in">open</span>(<span class="string">&#x27;xx.html&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>), <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据标签名查找节点，找到第一个符合条件的标签（找到第一个a）</span></span><br><span class="line"><span class="built_in">print</span>(soup.a)</span><br><span class="line"><span class="comment"># 获取标签的属性和属性值</span></span><br><span class="line"><span class="built_in">print</span>(soup.a.attrs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 三个函数</span></span><br><span class="line"><span class="comment"># (1)find</span></span><br><span class="line"><span class="comment"># 找到第一个a标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;a&#x27;</span>))</span><br><span class="line"><span class="comment"># 根据属性来查找a标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;a&#x27;</span>,<span class="built_in">id</span>=<span class="string">&#x27;名字&#x27;</span>))</span><br><span class="line"><span class="comment"># class查找标签时注意要加下划线</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;a&#x27;</span>,class_=<span class="string">&#x27;名字&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (2)find_all:返回一个列表</span></span><br><span class="line"><span class="comment"># 查找所有的a标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.find_all(<span class="string">&#x27;a&#x27;</span>))</span><br><span class="line"><span class="comment"># 查找多个标签：传入的参数是一个列表</span></span><br><span class="line"><span class="built_in">print</span>(soup.find_all([<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;li&#x27;</span>]))</span><br><span class="line"><span class="comment"># limit参数: 前几个</span></span><br><span class="line"><span class="built_in">print</span>(soup.find_all(<span class="string">&#x27;li&#x27;</span>,limit=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (3)select</span></span><br><span class="line"><span class="comment"># select方法：返回一个列表，多个数据</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;a&#x27;</span>))</span><br><span class="line"><span class="comment"># .(点)代表class找到class属性值,#代表id</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.a1&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;#a4&#x27;</span>))</span><br><span class="line"><span class="comment"># li中所有的class，id或者查找固定class=a1,id=a4</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;li[class]&#x27;</span>))</span><br><span class="line"><span class="comment"># 后代选择器：div下的所有li</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;div li&#x27;</span>))</span><br><span class="line"><span class="comment"># 子代选择器: ul下的子标签div</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;ul &gt; div &#x27;</span>))</span><br><span class="line"><span class="comment"># 找到所有的a和li标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;a,li&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 节点信息</span></span><br><span class="line"><span class="comment"># 获取id=a1的对象内容：get</span></span><br><span class="line">obj = soup.select(<span class="string">&#x27;#a4&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># 注意：select返回的为一个列表，要具体到对象</span></span><br><span class="line"><span class="built_in">print</span>(obj.get_text())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 节点属性</span></span><br><span class="line"><span class="comment"># 获取标签的名字</span></span><br><span class="line"><span class="built_in">print</span>(obj.name)</span><br><span class="line"><span class="comment"># 以字典形式放回标签的属性</span></span><br><span class="line"><span class="built_in">print</span>(obj.attrs)</span><br><span class="line"><span class="comment"># 获取节点属性</span></span><br><span class="line"><span class="built_in">print</span>(obj.attrs.get(<span class="string">&#x27;class&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="selenium"><a class="markdownIt-Anchor" href="#selenium">#</a> selenium</h2>
<ul>
<li>概念：基于浏览器自动化的模块</li>
<li>自动化：可以通过代码指定一些列的行为动作，然后将其作用到浏览器中。</li>
<li>pip install selenium</li>
<li>seleniurm 和爬虫之间的关联
<ul>
<li>1. 便捷的捕获到任意形式动态加载的数据 (可见即可得).</li>
<li>2 实现模拟登录</li>
</ul>
</li>
<li>谷歌驱动下载:<a class="link"   target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html" >http://chromedriver.storage.googleapis.com/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<hr>
<br>
<h2 id="requests"><a class="markdownIt-Anchor" href="#requests">#</a> requests</h2>
<ul>
<li>基本使用：一个类型，六个属性</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="comment"># 一个类型，六个属性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># response类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置响应的编码格式</span></span><br><span class="line">response.encoding=<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># text:以字符串形式返回网页源码</span></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取请求的url</span></span><br><span class="line"><span class="built_in">print</span>(response.url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回二进制数据</span></span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取标头</span></span><br><span class="line"><span class="built_in">print</span>(response.headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应状态码</span></span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>get 请求</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>:<span class="string">&#x27;北京&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.24&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url=url, params=data, headers=headers)</span><br><span class="line">response.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">content = response.text</span><br><span class="line"><span class="built_in">print</span>(content)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<ul>
<li>post 请求</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/v2transapi?from=en&amp;to=zh&#x27;</span></span><br><span class="line"><span class="comment">#有时要加cookie</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;eye&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;simple_means_flag&#x27;</span>: <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;67056.386753&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;token&#x27;</span>: <span class="string">&#x27;d3182f8b6953ad11628cc4fdca4f7194&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;common&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line">content = response.text</span><br><span class="line">obj = json.loads(content)</span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>代理</li>
</ul>
<h2 id="-code30-"><a class="markdownIt-Anchor" href="#-code30-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>: </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;ip&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">proxy = &#123;</span><br><span class="line">    <span class="string">&#x27;HTTP&#x27;</span>: <span class="string">&#x27;39.108.101.55:1080&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url=url,params=data,headers=headers,proxies=proxy)</span><br><span class="line">content = response.text</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;ip.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">f.write(content)</span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="scrapy"><a class="markdownIt-Anchor" href="#scrapy">#</a> scrapy</h2>
<ul>
<li>
<p>专门用于异步爬虫的框架</p>
</li>
<li>
<p>基本结构</p>
<ul>
<li>
<p>scrapy 项目框架</p>
<ul>
<li><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/10.png"
                      alt=""
                ></li>
</ul>
</li>
<li>
<p>创建爬虫项目</p>
<ul>
<li>scrapy startproject 项目名字</li>
<li>注意项目名字不能数字开头，不能包含中文</li>
</ul>
</li>
<li>
<p>创建爬虫文件</p>
<ul>
<li>在 spiders 文件夹中去创建爬虫文件<br>
 cd 项目名称 \ 项目名称 \spiders</li>
<li>创建爬虫文件</li>
<li>scrapy genspider 爬虫文件的名字 爬取的网页</li>
</ul>
</li>
<li>
<p>运行爬虫文件（<a class="link"   target="_blank" rel="noopener" href="http://xn-----settings-ix1s08cv1az40fnmuf79bmmc423t.py" >注意取消君子协议 ---settings.py <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
<ul>
<li>scrapy crawl 爬虫文件名字</li>
</ul>
</li>
<li>
<p>工作原理图<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/12.jpg"
                      alt=""
                ></p>
</li>
<li>
<p>scrapy shell<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/13.jpg"
                      alt=""
                ></p>
</li>
</ul>
</li>
<li>
<p><a class="link"   target="_blank" rel="noopener" href="http://xn--setting-wi0ml34iky4c3c1b.py" >基本配置 setting.py <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<ul>
<li>指定日志类型：LOG_LEVEL = “ERROR”</li>
<li>UA 伪装</li>
<li>禁用 robots</li>
</ul>
</li>
<li>
<p>内容<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/34.jpg"
                      alt=""
                ></p>
</li>
<li>
<p>scrapy 解析数据</p>
<ul>
<li>使用:response.xpath (‘xpath 表达式’)</li>
<li>Scrapy 封装的 xpath 和 etree 中的 xpath 区别:
<ul>
<li>scrapy 中的 xpath 直接将定位到的标签中存储的值或者属性值取出。返回的是 Selector 对象，<br>
数据值是存储在 Selector 对象的 data 属性中，需要调用 extract、extract_first () 取出<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/109.png"
                      alt=""
                ></li>
</ul>
</li>
</ul>
</li>
<li>
<p>持久化存储</p>
<ul>
<li>基于终端指令的持久化存储
<ul>
<li>要求：该种方式只可以将 parse 方法的返回值存储到本地指定后缀的文本文件中。</li>
<li>执行指令:scrapy crawl spiderName -o filePath</li>
</ul>
</li>
<li>基于管道的持久化存储 (重点)
<ul>
<li>在爬虫文件中进行数据解析</li>
<li>在 items.py 中定义相关属性
<ul>
<li>步骤 1 中解析出了几个字段的数据，在此就定义几个属性</li>
<li>在爬虫文件中将解析到的数据存储封装到 Item 类型的对象中</li>
<li>将 Item 类型的对象提交给管道</li>
<li>在管道文件 (<a class="link"   target="_blank" rel="noopener" href="http://pipelines.py" >pipelines.py <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>) 中，接收爬虫文件提交过来的 Item 类型对象，且对其进行持久化存储操作</li>
<li>在配置文件中开启管道<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/110.png"
                      alt=""
                >机制</li>
</ul>
</li>
</ul>
</li>
<li>基于管道实现数据的备份
<ul>
<li>将爬取到的数据分别存储到不同的载体</li>
<li>实现：一份到 mysql（导入 pymysql 模块），一份在 redis
<ul>
<li>一个管道类对应一种形式的持久化存储操作。如果将数据存储到不同的载体中就需要使用多个管道类</li>
<li>item 不会一次提交给三个管道类，只会被提交给管道优先级高的中去</li>
<li>优先级高的管道类需要在 process——item 中实现 return item，item 传递给下一个即将被执行的管道类<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/35.jpg"
                      alt=""
                ></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>手动请求  (针对需要进行多页请求)</p>
<ul>
<li>yield scrapy.Request(url, callback): GET
<ul>
<li>callback 指定解析函数，用于解析数据</li>
</ul>
</li>
<li>yield scrapy.FormRequest(url,callback, formdata):POST
<ul>
<li>formdata: 请求参数，字典</li>
</ul>
</li>
<li>为什么 start_urls 列表中的 url 会被自动进行 get 请求的发送？
<ul>
<li>因为列表中的 url 其实是被 start_requests 这个父类方法实现的 get 请求发送<br>
 def start_requests (self):<br>
for uin self.start urls:<br>
yield scrapy.Request(url=u, callback=self.parse)</li>
</ul>
</li>
<li>如何将 start_urls 中的 url 默认进行 post 请求的发送？
<ul>
<li>重写 start_requests 方法即可<br>
 def start-requests (self):<br>
for u in self.start_urls:<br>
yield scrapy.FromRequest(url=u, callback=self.parse,formdsata=data)<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/111.png"
                      alt=""
                ></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<br>
<h2 id="当当网案例"><a class="markdownIt-Anchor" href="#当当网案例">#</a> 当当网案例</h2>
<ul>
<li>业务逻辑</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy_dangdang.items <span class="keyword">import</span> ScrapyDangdangItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;dang&#x27;</span></span><br><span class="line">    <span class="comment"># 如果多页下载的，要调整allowed_domains的范围，一般只写域名</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;category.dangdang.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://category.dangdang.com/cp01.01.02.00.00.00.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    base_url = <span class="string">&#x27;http://category.dangdang.com/cp&#x27;</span></span><br><span class="line">    page = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    li_list = response.xpath(<span class="string">&#x27;//ul[@id=&quot;component_59&quot;]/li&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">        src = li.xpath(<span class="string">&#x27;.//img/@data-original&#x27;</span>).extract_first()</span><br><span class="line">        name = li.xpath(<span class="string">&#x27;.//img/@alt&#x27;</span>).extract_first()</span><br><span class="line">        price = li.xpath(<span class="string">&#x27;.//p[@class=&quot;price&quot;]/span[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> src:</span><br><span class="line">            src = src</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            src = li.xpath(<span class="string">&#x27;.//img/@src&#x27;</span>).extract_first()</span><br><span class="line">        <span class="built_in">print</span>(src, name, price)</span><br><span class="line">        <span class="comment"># 每一本书一个book对象</span></span><br><span class="line">        book = ScrapyDangdangItem(src=src, name=name, price=price)</span><br><span class="line">        <span class="comment"># 获取一个book就将他交给pipelines下载</span></span><br><span class="line">        <span class="keyword">yield</span> book</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每一页的业务逻辑都是一样的，我们只需要将每一页的请求再次调用parse方法</span></span><br><span class="line">        <span class="comment"># 寻找每一页url的共同点</span></span><br><span class="line">        <span class="comment"># http://category.dangdang.com/pg2-cp01.01.02.00.00.00.html</span></span><br><span class="line">        <span class="comment"># http://category.dangdang.com/pg3-cp01.01.02.00.00.00.html</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.page &lt; <span class="number">100</span>:</span><br><span class="line">        <span class="comment"># page加1</span></span><br><span class="line">        self.page += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 更新url</span></span><br><span class="line">        url = self.base_url + <span class="built_in">str</span>(self.page) + <span class="string">&#x27;-cp01.01.02.00.00.00.html&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 怎么去调用parse方法</span></span><br><span class="line">        <span class="comment"># scrapy.Request就是scrapy的get请求</span></span><br><span class="line">        <span class="comment"># url：请求地址</span></span><br><span class="line">        <span class="comment"># callback：需要执行的函数，不要加（）</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=url,callback=self.parse)</span><br></pre></td></tr></table></figure></div>
<ul>
<li>多页下载</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果多页下载的，要调整allowed_domains的范围，一般只写域名</span></span><br><span class="line">allowed_domains = [<span class="string">&#x27;http://category.dangdang.com/cp01.01.02.00.00.00.html&#x27;</span>]</span><br><span class="line">start_urls = [<span class="string">&#x27;http://category.dangdang.com/cp01.01.02.00.00.00.html&#x27;</span>]</span><br></pre></td></tr></table></figure></div>
<ul>
<li>定义数据结构：<a class="link"   target="_blank" rel="noopener" href="http://itme.py" >itme.py <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 文件中</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScrapyDangdangItem</span>(scrapy.Item):</span><br><span class="line">    src = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br></pre></td></tr></table></figure></div>
<ul>
<li>开启管道：setting.py 中</li>
<li>封装管道</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 如果想使用管道就需要在setting中开启管道</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScrapyDangdangPipeline</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫文件开始之前就会执行的一个方法，只执行一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;book.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># itme就是yield后面的book对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 不推荐此方法：文件打开过于频繁，每传入一个itme就打开依次文件</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># write方法是写入一个字符串，而不能是一个对象</span></span><br><span class="line">        <span class="comment"># w模式会将之前写入的内容覆盖，因此要使用追加模式a</span></span><br><span class="line">        <span class="comment"># with open(&#x27;book.json&#x27;,&#x27;a&#x27;,encoding=&#x27;utf-8&#x27;) as fp:</span></span><br><span class="line">        <span class="comment">#     fp.write(str(item))</span></span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫文件结束时就会执行的一个方法，只执行一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure></div>
<ul>
<li>开启多条管道</li>
</ul>
<h2 id="-code35-"><a class="markdownIt-Anchor" href="#-code35-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启多条管道</span></span><br><span class="line"><span class="comment"># （1）定义管道类</span></span><br><span class="line"><span class="comment"># （2）在setting中开启管道</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangDangtupiandownloadPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        url = <span class="string">&#x27;http:&#x27;</span> + item.get(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">        name = <span class="string">&#x27;./book/&#x27;</span> + item.get(<span class="string">&#x27;name&#x27;</span>) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">        urllib.request.urlretrieve(url=url, filename=name)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="电影天堂案例"><a class="markdownIt-Anchor" href="#电影天堂案例">#</a> 电影天堂案例</h2>
<ul>
<li>重要：第一页的名字和第二页的图片，需要写 prase_seconde—&gt; 使用 meta 传参</li>
<li>业务逻辑</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy_movie.items <span class="keyword">import</span> ScrapyMovieItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MvSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;mv&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.ygdy8.net&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.ygdy8.net/html/gndy/china/index.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 要第一页的名字和   第二页的图片</span></span><br><span class="line">        a_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;co_content8&quot;]//td[2]//a[2]&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> a_list:</span><br><span class="line">            <span class="comment"># 获取第一页的名字   第二页的连接</span></span><br><span class="line">            href = a.xpath(<span class="string">&#x27;./@href&#x27;</span>).extract_first()</span><br><span class="line">            name = a.xpath(<span class="string">&#x27;./text()&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 第二页的地址需要拼接</span></span><br><span class="line">            url = <span class="string">&#x27;https://www.ygdy8.net&#x27;</span> + href</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对第二页进行访问: 无限嵌套</span></span><br><span class="line">            <span class="comment"># 注意：第二页的地址是否在允许访问的范围之内   allowed_domains：一般只写域名</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url,callback=self.parse_second,meta=&#123;<span class="string">&#x27;name&#x27;</span>:name&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_second</span>(<span class="params">self,response</span>):</span><br><span class="line">        <span class="comment"># 如果拿不到数据，检查xpath语法，有可能识别不了span</span></span><br><span class="line">        src = response.xpath(<span class="string">&#x27;//div[@id=&quot;Zoom&quot;]//img/@src&#x27;</span>).extract_first()</span><br><span class="line">        <span class="comment"># meta用来传递prase中的name值</span></span><br><span class="line">        name = response.meta[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 需要导包</span></span><br><span class="line">        movie = ScrapyMovieItem(src=src,name=name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># movie返回给管道</span></span><br><span class="line">        <span class="keyword">yield</span> movie</span><br></pre></td></tr></table></figure></div>
<ul>
<li>数据结构定义</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScrapyMovieItem</span>(scrapy.Item):</span><br><span class="line"><span class="comment"># define the fields for your item here like:</span></span><br><span class="line"><span class="comment"># name = scrapy.Field()</span></span><br><span class="line"></span><br><span class="line">src = scrapy.Field()</span><br><span class="line">name = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<hr>
<br>
<h2 id="crawlspider"><a class="markdownIt-Anchor" href="#crawlspider">#</a> crawlspider</h2>
<ul>
<li>介绍<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/14.jpg"
                      alt=""
                ></li>
<li>提取链接<br>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/image/15.jpg"
                      alt=""
                ></li>
<li>读书网案例</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy_dushuwang.items <span class="keyword">import</span> ScrapyDushuwangItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReadSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;read&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.dushu.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.dushu.com/book/1188_1.html&#x27;</span>]   <span class="comment"># 主注意第一页</span></span><br><span class="line"></span><br><span class="line">    rules = (                                   <span class="comment"># \.使点号转义</span></span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/book/1188_\d+\.html&#x27;</span>),</span><br><span class="line">        <span class="comment"># extracr_xpath = r&#x27;//div[@class=&quot;pages&quot;]/a/@href&#x27;</span></span><br><span class="line">                        callback=<span class="string">&#x27;parse_item&#x27;</span>,</span><br><span class="line">                        follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        img_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;bookslist&quot;]//li//img&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> img <span class="keyword">in</span> img_list:</span><br><span class="line">            src = img.xpath(<span class="string">&#x27;./@data-original&#x27;</span>).extract_first()</span><br><span class="line">            name = img.xpath(<span class="string">&#x27;./@alt&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            book = ScrapyDushuwangItem(src=src,name=name)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> book</span><br></pre></td></tr></table></figure></div>
<br>
<h2 id="scrapypost请求"><a class="markdownIt-Anchor" href="#scrapypost请求">#</a> scrapy–post 请求</h2>
<ul>
<li></li>
</ul>
<h2 id="-code39-"><a class="markdownIt-Anchor" href="#-code39-">#</a> <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestpostSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;testpost&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;fanyi.baidu.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># post请求 如果没有参数 那么就没有意义</span></span><br><span class="line">    <span class="comment"># 所以start_urls 页没有意义</span></span><br><span class="line">    <span class="comment"># parse方法也没有用</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># start_urls = [&#x27;http://fanyi.baidu.com/&#x27;]</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># def parse(self, response):</span></span><br><span class="line">    <span class="comment">#     pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        url = <span class="string">&#x27;https://fanyi.baidu.com/v2transapi?&#x27;</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;spider&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;from&#x27;</span>:<span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;to&#x27;</span>:<span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;simple_means_flag&#x27;</span>:<span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;sign&#x27;</span>:<span class="string">&#x27;63766.268839&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;token&#x27;</span>:<span class="string">&#x27;d3182f8b6953ad11628cc4fdca4f7194&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;domain&#x27;</span>:<span class="string">&#x27;common&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 发起请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_second</span>(<span class="params">self, response</span>):</span><br><span class="line">        content = response.text</span><br><span class="line">        obj = json.loads(content)</span><br><span class="line">        <span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure></div></h2>
<br>
<h2 id="scrapy中数据库操作"><a class="markdownIt-Anchor" href="#scrapy中数据库操作">#</a> scrapy 中数据库操作</h2>
<ul>
<li>管道中操作</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pymysql <span class="keyword">import</span> Connection</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScrapyUpfoxPipeline</span>:</span><br><span class="line"><span class="comment"># 启动spider自动执行</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.conn = Connection(</span><br><span class="line">        host=<span class="string">&quot;localhost&quot;</span>,  <span class="comment"># 主机名</span></span><br><span class="line">        port=<span class="number">3306</span>,  <span class="comment"># 端口号</span></span><br><span class="line">        user=<span class="string">&quot;root&quot;</span>,  <span class="comment"># 用户名</span></span><br><span class="line">        password=<span class="string">&#x27;123456&#x27;</span>, <span class="comment"># 密码</span></span><br><span class="line">        charset=<span class="string">&#x27;utf8&#x27;</span>,</span><br><span class="line">        db=<span class="string">&quot;scrapy&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 创建操作数据的游标</span></span><br><span class="line">    self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    name = item[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">    price = item[<span class="string">&#x27;price&#x27;</span>]</span><br><span class="line">    src = item[<span class="string">&#x27;src&#x27;</span>]</span><br><span class="line">    self.cursor.execute(</span><br><span class="line">        <span class="string">&#x27;insert into app1_upfox_show (name, img, price) values (&quot;&#123;&#125;&quot;, &quot;&#123;&#125;&quot;, &quot;&#123;&#125;&quot;)&#x27;</span>.<span class="built_in">format</span>(name, src, price)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.conn.commit()</span><br><span class="line">    <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">    self.cursor.close()</span><br><span class="line">    self.conn.close()</span><br></pre></td></tr></table></figure></div>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>本文标题：爬虫</li>
        <li>本文作者：Xu</li>
        <li>创建时间：2022-10-29 17:08:03</li>
        <li>
            本文链接：http://example.com/2022/10/29/爬虫/
        </li>
        <li>
            版权声明：本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/Python/">#Python</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2022/11/11/MySQL/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">MySQL</span>
                                    <span class="post-nav-item">上一篇</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/10/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">数据结构</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">爬虫</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#12306%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E7%BC%96%E7%A0%81%E6%B5%81%E7%A8%8B"><span class="nav-text"> 12306 模拟登录编码流程:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%A8%E8%A7%86%E9%A2%91%E7%88%AC%E5%8F%96%E6%80%9D%E8%B7%AF"><span class="nav-text"> 梨视频爬取思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="nav-text"> 异步爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-text"> 分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#request"><span class="nav-text"> request</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#urllib"><span class="nav-text"> urllib</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ua%E5%8F%8D%E7%88%AC"><span class="nav-text"> UA 反爬</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="nav-text"> 图片下载两种方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#urllibparesquote%E6%96%B9%E6%B3%95%E5%B0%86%E6%B1%89%E5%AD%97%E8%BD%AC%E6%8D%A2%E6%88%90unicode%E7%BC%96%E7%A0%81"><span class="nav-text"> urllib.pares.quote () 方法：将汉字转换成 unicode 编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#urllibparseurlencode%E5%B0%86%E5%A4%9A%E4%B8%AA%E5%8F%82%E6%95%B0%E5%90%8C%E6%97%B6%E8%BD%AC%E6%8D%A2%E6%88%90unicode%E7%BC%96%E7%A0%81%E5%B9%B6%E4%B8%94%E7%94%A8%E8%BF%9B%E8%A1%8C%E8%BF%9E%E6%8E%A5"><span class="nav-text"> urllib.parse.urlencode ()：将多个参数同时转换成 unicode 编码并且用 &amp; 进行连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#post%E8%AF%B7%E6%B1%82"><span class="nav-text"> post 请求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ajax-get%E8%AF%B7%E6%B1%82%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96"><span class="nav-text"> ajax get 请求（豆瓣电影数据获取）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ajax-post%E8%AF%B7%E6%B1%82%E8%82%AF%E5%BE%B7%E5%9F%BA%E6%89%BE%E5%87%BA%E6%AF%8F%E9%A1%B5%E7%9A%84%E8%A7%84%E5%BE%8B"><span class="nav-text"> ajax post 请求 — 肯德基 (找出每页的规律)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code12-"><span class="nav-text"> 123456789101112131415161718192021222324252627282930313233import urllib.requestimport urllib.parsedef creat_request(page):    url&#x3D;&quot;http:&#x2F;&#x2F;www.kfc.com.cn&#x2F;kfccda&#x2F;ashx&#x2F;GetStoreList.ashx?op&#x3D;cname&quot;    headers &#x3D; {        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;107.0.0.0 Safari&#x2F;537.36 Edg&#x2F;107.0.1418.24&#39;    }    data&#x3D;{        &#39;cname&#39;: &#39;重庆&#39;,        &#39;pid&#39;: &#39; &#39;,        &#39;pageIndex&#39;: page,        &#39;pageSize&#39;: &quot;10&quot;    }    data&#x3D;urllib.parse.urlencode(data).encode(&#39;utf-8&#39;)    request&#x3D;urllib.request.Request(url&#x3D;url,headers&#x3D;headers,data&#x3D;data)    return requestdef down_load(page,content):    f&#x3D;open(&quot;地址_&quot;+str(page)+&#39;.json&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;)    f.write(content)if __name__ &#x3D;&#x3D; &#39;__main__&#39;:    start_page&#x3D;int(input(&#39;请输入开始页码: &#39;))    end_page&#x3D;int(input(&#39;请输入结束页码: &#39;))    for page in range(start_page,end_page+1):        #定制每一页请求对象        request &#x3D; creat_request(page)        #获取相应        response&#x3D;urllib.request.urlopen(request)        #接收内容        content&#x3D;response.read().decode(&quot;utf-8&quot;)        #下载        down_load(page,content)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8"><span class="nav-text"> 异常</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cookie"><span class="nav-text"> cookie</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code15-"><span class="nav-text"> 12345678910111213141516171819202122232425262728import urllib.requesturl&#x3D;&quot;https:&#x2F;&#x2F;user.qzone.qq.com&#x2F;2578393167&#x2F;infocenter&quot;headers&#x3D;{    &#39;accept&#39;: &#39;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9&#39;,    &#39;accept-language&#39;: &#39;zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8,en-GB;q&#x3D;0.7,en-US;q&#x3D;0.6&#39;,    &#39;cache-control&#39;: &#39;max-age&#x3D;0&#39;,    &#39;cookie&#39;: &#39;2578393167_todaycount&#x3D;0; 2578393167_totalcount&#x3D;2183; RK&#x3D;YVvUR0JQ12; ptcz&#x3D;c627979381dfc7a30c18832d7fb11cc2a5570256a6943af7e90b25439b6c0feb; pgv_pvid&#x3D;8367386128; tvfe_boss_uuid&#x3D;bf664e715257f060; o_cookie&#x3D;2578393167; qz_screen&#x3D;1536x864; QZ_FE_WEBP_SUPPORT&#x3D;1; uin&#x3D;o2578393167; skey&#x3D;@gkZqZO2fy; p_uin&#x3D;o2578393167; Loading&#x3D;Yes; pgv_info&#x3D;ssid&#x3D;s8446429120; pt4_token&#x3D;FR7N-PhJ3XrcHbADF9QUKxTeM18cRk0qudgiPqWD1lU_; p_skey&#x3D;3A-LodRdokqoPcKgd0VuwAOOtAPDi7kntBosX8jeieI_; cpu_performance_v8&#x3D;3&#39;,    &#39;if-modified-since&#39;: &#39;Wed, 02 Nov 2022 15:11:28 GMT&#39;,    &#39;sec-ch-ua&#39;:&#39; &quot;Microsoft Edge&quot;;v&#x3D;&quot;107&quot;, &quot;Chromium&quot;;v&#x3D;&quot;107&quot;, &quot;Not&#x3D;A?Brand&quot;;v&#x3D;&quot;24&quot;&#39;,    &#39;sec-ch-ua-mobile&#39;: &#39;?0&#39;,    &#39;sec-ch-ua-platform&#39;: &quot;Windows&quot;,    &#39;sec-fetch-dest&#39;: &#39;document&#39;,    &#39;sec-fetch-mode&#39;: &#39;navigate&#39;,    &#39;sec-fetch-site&#39;: &#39;none&#39;,    &#39;sec-fetch-user&#39;: &#39;?1&#39;,    &#39;upgrade-insecure-requests&#39;: &#39;1&#39;,    &#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;107.0.0.0 Safari&#x2F;537.36 Edg&#x2F;107.0.1418.26&#39;    }request &#x3D; urllib.request.Request(url&#x3D;url,headers&#x3D;headers)response &#x3D; urllib.request.urlopen(request)content &#x3D; response.read().decode(&#39;utf-8&#39;)f &#x3D; open(&#39;wangye.html&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;)f.write(content)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#handler%E5%A4%84%E7%90%86"><span class="nav-text"> Handler 处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xpath%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-text"> xpath 基本使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code23-"><span class="nav-text"> 12345678910111213141516171819202122232425262728293031323334353637383940414243from lxml import etreeimport urllib.request# https:&#x2F;&#x2F;sc.chinaz.com&#x2F;tupian&#x2F;fengjing.html# https:&#x2F;&#x2F;sc.chinaz.com&#x2F;tupian&#x2F;fengjing_2.htmldef creat_request(page):    if(page  &#x3D;&#x3D; 1):        url &#x3D; &#39;https:&#x2F;&#x2F;sc.chinaz.com&#x2F;tupian&#x2F;fengjing.html&#39;    else:        url &#x3D; &#39;https:&#x2F;&#x2F;sc.chinaz.com&#x2F;tupian&#x2F;fengjing_&#39; +str(page)+ &#39;.html&#39;    headers &#x3D; {            &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;107.0.0.0 Safari&#x2F;537.36 Edg&#x2F;107.0.1418.24&#39;        }    request &#x3D; urllib.request.Request(url&#x3D;url, headers&#x3D;headers)    return request    def get_content(request):    response &#x3D; urllib.request.urlopen(request)    content &#x3D; response.read().decode(&#39;utf-8&#39;)    return contentdef down_load(content):    #下载图片    tree &#x3D; etree.HTML(content)    url_list &#x3D; tree.xpath(&#39;&#x2F;html&#x2F;body&#x2F;div[3]&#x2F;div[2]&#x2F;&#x2F;img&#x2F;@data-original&#39;)    name_list &#x3D; tree.xpath(&#39;&#x2F;html&#x2F;body&#x2F;div[3]&#x2F;div[2]&#x2F;&#x2F;img&#x2F;@alt&#39;)    for i in range(len(name_list)):        url &#x3D; url_list[i]        name &#x3D; name_list[i]        url &#x3D; &#39;http:&#39; + url_list[i]        urllib.request.urlretrieve(url&#x3D;url, filename&#x3D;&#39;.&#x2F;s&#x2F;&#39;+name+&#39;.jpg&#39;)if __name__ &#x3D;&#x3D; &#39;__main__&#39;:    start_page &#x3D; int(input(&#39;请输入开始页码&#39;))    end_page &#x3D; int(input(&quot;输入结束页码：&quot;))    for page in range(start_page,end_page+1):# 请求对象定制        request &#x3D; creat_request(page)#  获取网页源码        content &#x3D; get_content(request)# 下载        down_load(content)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jsonpath"><span class="nav-text"> JsonPath</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bs4"><span class="nav-text"> bs4</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code26-"><span class="nav-text"> 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from bs4 import BeautifulSoup# 默认打开文件的编码为gbk，所以在打开文件时要指定编码格式soup &#x3D; BeautifulSoup(open(&#39;xx.html&#39;, encoding&#x3D;&#39;utf-8&#39;), &#39;lxml&#39;)# 根据标签名查找节点，找到第一个符合条件的标签（找到第一个a）print(soup.a)# 获取标签的属性和属性值print(soup.a.attrs)# 三个函数# (1)find# 找到第一个a标签print(soup.find(&#39;a&#39;))# 根据属性来查找a标签print(soup.find(&#39;a&#39;,id&#x3D;&#39;名字&#39;))# class查找标签时注意要加下划线print(soup.find(&#39;a&#39;,class_&#x3D;&#39;名字&#39;))# (2)find_all:返回一个列表# 查找所有的a标签print(soup.find_all(&#39;a&#39;))# 查找多个标签：传入的参数是一个列表print(soup.find_all([&#39;a&#39;,&#39;li&#39;]))# limit参数: 前几个print(soup.find_all(&#39;li&#39;,limit&#x3D;2))# (3)select# select方法：返回一个列表，多个数据print(soup.select(&#39;a&#39;))# .(点)代表class找到class属性值,#代表idprint(soup.select(&#39;.a1&#39;))print(soup.select(&#39;#a4&#39;))# li中所有的class，id或者查找固定class&#x3D;a1,id&#x3D;a4print(soup.select(&#39;li[class]&#39;))# 后代选择器：div下的所有liprint(soup.select(&#39;div li&#39;))# 子代选择器: ul下的子标签divprint(soup.select(&#39;ul &gt; div &#39;))# 找到所有的a和li标签print(soup.select(&#39;a,li&#39;))# 节点信息# 获取id&#x3D;a1的对象内容：getobj &#x3D; soup.select(&#39;#a4&#39;)[0]  # 注意：select返回的为一个列表，要具体到对象print(obj.get_text())# 节点属性# 获取标签的名字print(obj.name)# 以字典形式放回标签的属性print(obj.attrs)# 获取节点属性print(obj.attrs.get(&#39;class&#39;))</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#selenium"><span class="nav-text"> selenium</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#requests"><span class="nav-text"> requests</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code30-"><span class="nav-text"> 123456789101112131415161718import requestsurl &#x3D; &#39;http:&#x2F;&#x2F;www.baidu.com&#x2F;s?&#39;headers &#x3D; {    &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;107.0.0.0 Safari&#x2F;537.36 Edg&#x2F;107.0.1418.35&#39;,    &#39;Cookie&#39;: }data &#x3D; {    &#39;wd&#39;: &#39;ip&#39;}proxy &#x3D; {    &#39;HTTP&#39;: &#39;39.108.101.55:1080&#39;}response &#x3D; requests.get(url&#x3D;url,params&#x3D;data,headers&#x3D;headers,proxies&#x3D;proxy)content &#x3D; response.textf &#x3D; open(&#39;ip.html&#39;,&#39;w&#39;,encoding&#x3D;&#39;utf-8&#39;)f.write(content)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy"><span class="nav-text"> scrapy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%93%E5%BD%93%E7%BD%91%E6%A1%88%E4%BE%8B"><span class="nav-text"> 当当网案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code35-"><span class="nav-text"> 12345678910111213# 开启多条管道# （1）定义管道类# （2）在setting中开启管道import urllib.requestclass DangDangtupiandownloadPipeline:    def process_item(self, item, spider):        url &#x3D; &#39;http:&#39; + item.get(&#39;src&#39;)        name &#x3D; &#39;.&#x2F;book&#x2F;&#39; + item.get(&#39;name&#39;) + &#39;.jpg&#39;        urllib.request.urlretrieve(url&#x3D;url, filename&#x3D;name)        return item</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82%E6%A1%88%E4%BE%8B"><span class="nav-text"> 电影天堂案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#crawlspider"><span class="nav-text"> crawlspider</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapypost%E8%AF%B7%E6%B1%82"><span class="nav-text"> scrapy–post 请求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#-code39-"><span class="nav-text"> 123456789101112131415161718192021222324252627282930313233343536import scrapyimport jsonclass TestpostSpider(scrapy.Spider):    name &#x3D; &#39;testpost&#39;    allowed_domains &#x3D; [&#39;fanyi.baidu.com&#39;]    # post请求 如果没有参数 那么就没有意义    # 所以start_urls 页没有意义    # parse方法也没有用    # start_urls &#x3D; [&#39;http:&#x2F;&#x2F;fanyi.baidu.com&#x2F;&#39;]    #    # def parse(self, response):    #     pass    # 参数    def start_requests(self):        url &#x3D; &#39;https:&#x2F;&#x2F;fanyi.baidu.com&#x2F;v2transapi?&#39;        data &#x3D; {            &#39;query&#39;: &#39;spider&#39;,            &#39;from&#39;:&#39;en&#39;,            &#39;to&#39;:&#39;zh&#39;,            &#39;simple_means_flag&#39;:&#39;3&#39;,            &#39;sign&#39;:&#39;63766.268839&#39;,            &#39;token&#39;:&#39;d3182f8b6953ad11628cc4fdca4f7194&#39;,            &#39;domain&#39;:&#39;common&#39;        }        yield scrapy.FormRequest(url&#x3D;url, formdata&#x3D;data, callback&#x3D;self.parse_second)    # 发起请求    def parse_second(self, response):        content &#x3D; response.text        obj &#x3D; json.loads(content)        print(obj)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy%E4%B8%AD%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C"><span class="nav-text"> scrapy 中数据库操作</span></a></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-regular fa-computer-classic"></i>&nbsp;&nbsp;<a href="/">Xu</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br> 
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v</a>
        </div>
        
        
        
            <div id="start_time_div" style="display:none">
                2022/8/17 11:45:14
            </div>
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="unfolded-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="fa-regular fa-arrow-up"></i>
            </li>
        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="folded-tools-list">
        <li class="right-bottom-tools tool-toggle-show flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/menu-shrink.js"></script>

<script src="/js/tools/go-top-bottom.js"></script>

<script src="/js/tools/dark-light-toggle.js"></script>





    
<script src="/js/tools/code-block.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/layouts/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">





<div class="post-scripts pjax">
    
        
<script src="/js/tools/toc-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>




<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
